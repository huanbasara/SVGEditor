# Diffusion å›¾åƒç¼–è¾‘æ–¹æ¡ˆå‚è€ƒ 3 - æŒ‡ä»¤å¼ç¼–è¾‘æ¨¡å‹

## ğŸ¯ ç›®æ ‡

- **è¾“å…¥**ï¼šå›¾åƒï¼ˆinit imageï¼‰ + æŒ‡ä»¤ï¼ˆinstruction promptï¼‰
- **è¦æ±‚**ï¼šç²¾å‡†ç¼–è¾‘ï¼Œä¿æŒåŸå›¾é£æ ¼ï¼ˆå°¤å…¶æ˜¯åŠ¨ç”»/çº¯è‰²å—é£æ ¼ï¼‰
- **é—®é¢˜**ï¼šç°æœ‰ IP-Adapter æ–¹æ¡ˆä¸»è¦æ”¹å˜é¢œè‰²ï¼Œç»“æ„ç¼–è¾‘èƒ½åŠ›ä¸è¶³
- **æ–¹æ³•**ï¼šæ¢ç´¢ä¸“é—¨çš„æŒ‡ä»¤å¼ç¼–è¾‘æ¨¡å‹ï¼Œæå‡ç¼–è¾‘ç²¾å‡†åº¦

---

## ğŸ“¦ ç¯å¢ƒä¾èµ–

```bash
pip install -U diffusers transformers accelerate safetensors controlnet-aux
```

éœ€è¦ GPU (CUDA)ã€‚

---

## ğŸš€ æ¨èæ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### **ç¬¬ä¸€ä¼˜å…ˆçº§ï¼ˆå®¹æ˜“é›†æˆï¼‰**

#### **1. InstructPix2Pix ç³»åˆ—**
```python
# åŸºç¡€ç‰ˆæœ¬ï¼ˆå·²ä¸‹è½½ï¼‰
"timbrooks/instruct-pix2pix"

# SDXLç‰ˆæœ¬ï¼ˆæ›´é«˜è´¨é‡ï¼‰
"diffusers/sdxl-instructpix2pix-768"

# ç¤¾åŒºå¾®è°ƒç‰ˆæœ¬
"instruction-tuning/instruct-pix2pix-finetuned"
```

#### **2. å‚æ•°ä¼˜åŒ–çš„ IP-Adapter**
```python
# å½“å‰é—®é¢˜ï¼šç¼–è¾‘èƒ½åŠ›ä¸è¶³ï¼Œä¸»è¦æ”¹å˜é¢œè‰²
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´å‚æ•°å¹³è¡¡
strength=0.5,           # å¢åŠ ç¼–è¾‘å¼ºåº¦ï¼ˆå½“å‰0.3ï¼‰
guidance_scale=10.0,    # å¢å¼ºæ–‡æœ¬å¼•å¯¼ï¼ˆå½“å‰7.0ï¼‰
ip_adapter_scale=0.3,   # é™ä½é£æ ¼æ§åˆ¶ï¼ˆå½“å‰0.8ï¼‰
```

### **ç¬¬äºŒä¼˜å…ˆçº§ï¼ˆä¸­ç­‰å¤æ‚åº¦ï¼‰**

#### **3. InstructPix2Pix + ControlNet ç»„åˆ**
```python
# ç»“åˆæŒ‡ä»¤ç¼–è¾‘ + ç»“æ„ä¿æŒ
pipeline = StableDiffusionInstructPix2PixPipeline
controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny")
```

#### **4. ä¸“é—¨çš„ç¼–è¾‘æ¨¡å‹**
```python
# å±€éƒ¨ç¼–è¾‘ä¸“ç”¨
"ByteDance/MagicEdit"

# å¢å¼ºæŒ‡ä»¤ç†è§£
"instruction-tuning/InstructDiffusion"

# SAMç²¾ç¡®ç¼–è¾‘
"ShilongLiu/EditAnything"
```

### **ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼ˆæœ€æ–°æŠ€æœ¯ï¼‰**

#### **5. é£æ ¼ä¿æŒ + æŒ‡ä»¤ç¼–è¾‘**
```python
# Googleé£æ ¼ä¿æŒç¼–è¾‘
"google/styledrop-sd15"

# ç»“åˆé£æ ¼ä¿æŒå’ŒæŒ‡ä»¤ç¼–è¾‘
"DreamEdit/dreamedit-sd15"
```

#### **6. æœ€æ–°å¼ºåŠ›æ¨¡å‹**
```python
# æœ€æ–°ç¼–è¾‘æ¨¡å‹
"black-forest-labs/FLUX.1-Fill-dev"

# SD3ç¼–è¾‘ç‰ˆæœ¬
"stabilityai/stable-diffusion-3-medium-edit"
```

---

## ğŸ”§ æµ‹è¯•æ–¹æ¡ˆ

### **æ–¹æ¡ˆ A: InstructPix2Pixï¼ˆæ¨èé¦–é€‰ï¼‰**

```python
from diffusers import StableDiffusionInstructPix2PixPipeline

pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
    "timbrooks/instruct-pix2pix",
    torch_dtype=torch.float16
).to("cuda")

result = pipe(
    prompt="Remove the leaves from the apple",
    image=pil_image_256,
    num_inference_steps=20,
    image_guidance_scale=1.5,  # ä¿æŒåŸå›¾ç›¸ä¼¼åº¦
    guidance_scale=7.5         # æ–‡æœ¬æŒ‡ä»¤å¼ºåº¦
).images[0]
```

### **æ–¹æ¡ˆ B: ä¼˜åŒ–çš„ IP-Adapter**

```python
result = pipe(
    prompt=edit_prompt,  # ç®€åŒ–promptï¼Œåªç”¨ç¼–è¾‘æŒ‡ä»¤
    negative_prompt="purple background, colored background",
    image=pil_image_256,
    ip_adapter_image=style_reference,
    strength=0.6,        # å¢åŠ ç¼–è¾‘å¼ºåº¦
    guidance_scale=12.0, # å¢å¼ºæ–‡æœ¬å¼•å¯¼
    ip_adapter_scale=0.2, # å¤§å¹…é™ä½é£æ ¼æ§åˆ¶
    num_inference_steps=30
)
```

### **æ–¹æ¡ˆ C: ControlNet + IP-Adapter åŒé‡æ§åˆ¶**

```python
# åŠ è½½ ControlNet ä¿æŒç»“æ„
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11p_sd15s2_lineart_anime"
)

pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet
)

# åŠ è½½ IP-Adapter ä¿æŒé£æ ¼
pipe.load_ip_adapter("h94/IP-Adapter", weight_name="ip-adapter_sd15.bin")

# ç”Ÿæˆè¾¹ç¼˜æ§åˆ¶å›¾
import cv2
edges = cv2.Canny(np.array(pil_image_256), 50, 150)
control_image = Image.fromarray(edges)

result = pipe(
    prompt=edit_prompt,
    image=pil_image_256,
    control_image=control_image,
    ip_adapter_image=pil_image_256,
    strength=0.4,
    controlnet_conditioning_scale=0.8,
    ip_adapter_scale=0.5
)
```

---

## âš™ï¸ è°ƒå‚ç­–ç•¥

### **é’ˆå¯¹ç»“æ„ç¼–è¾‘ä¸è¶³çš„é—®é¢˜**

| å‚æ•° | å½“å‰å€¼ | å»ºè®®å€¼ | è¯´æ˜ |
|------|--------|--------|------|
| `strength` | 0.3 | 0.5-0.6 | å¢åŠ ç¼–è¾‘å¼ºåº¦ |
| `guidance_scale` | 7.0 | 10.0-12.0 | å¢å¼ºæ–‡æœ¬æŒ‡ä»¤æƒé‡ |
| `ip_adapter_scale` | 0.8 | 0.2-0.4 | é™ä½é£æ ¼æ§åˆ¶ï¼Œè®©ç¼–è¾‘æ›´è‡ªç”± |
| `num_inference_steps` | 25 | 30-40 | å¢åŠ æ¨ç†ç²¾åº¦ |

### **é’ˆå¯¹é¢œè‰²åç§»çš„é—®é¢˜**

```python
negative_prompt = "purple background, colored background, wrong colors, color shift, tinted"
```

---

## ğŸ” æ¨¡å‹ä¸‹è½½é…ç½®

### **æ›´æ–° models_config**

```python
models_config = {
    # ç°æœ‰æ¨¡å‹
    "instruct-pix2pix": "timbrooks/instruct-pix2pix",
    "stable-diffusion-v1-5": "runwayml/stable-diffusion-v1-5",
    "stable-diffusion-xl-base": "stabilityai/stable-diffusion-xl-base-1.0",
    "ip-adapter": "h94/IP-Adapter",
    
    # æ–°å¢æŒ‡ä»¤ç¼–è¾‘æ¨¡å‹
    "instruct-pix2pix-sdxl": "diffusers/sdxl-instructpix2pix-768",
    "controlnet-lineart-anime": "lllyasviel/control_v11p_sd15s2_lineart_anime",
    "magic-edit": "ByteDance/MagicEdit",
    
    # å®éªŒæ€§æ¨¡å‹
    "flux-edit": "black-forest-labs/FLUX.1-Fill-dev",
    "sd3-edit": "stabilityai/stable-diffusion-3-medium-edit"
}
```

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’

### **é˜¶æ®µ 1ï¼šå¿«é€ŸéªŒè¯**
1. è°ƒæ•´å½“å‰ IP-Adapter å‚æ•°
2. åˆ‡æ¢åˆ° InstructPix2Pix
3. å¯¹æ¯”æ•ˆæœå·®å¼‚

### **é˜¶æ®µ 2ï¼šç»„åˆæµ‹è¯•**
1. InstructPix2Pix + ControlNet
2. ä¸åŒ ControlNet ç±»å‹æµ‹è¯•
3. å‚æ•°ç½‘æ ¼æœç´¢

### **é˜¶æ®µ 3ï¼šæ–°æ¨¡å‹è¯„ä¼°**
1. SDXL InstructPix2Pix
2. MagicEdit ä¸“ç”¨ç¼–è¾‘
3. æœ€æ–° FLUX/SD3 æ¨¡å‹

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### **ç¼–è¾‘ç²¾å‡†åº¦**
- æ˜¯å¦æŒ‰æŒ‡ä»¤æ­£ç¡®ç¼–è¾‘ï¼ˆå¦‚çœŸçš„ç§»é™¤å¶å­ï¼‰
- ç¼–è¾‘åŒºåŸŸæ˜¯å¦ç²¾ç¡®
- æ˜¯å¦ä¿æŒå…¶ä»–åŒºåŸŸä¸å˜

### **é£æ ¼ä¿æŒåº¦**
- é¢œè‰²æ˜¯å¦ä¿æŒåŸæ ·
- è‰ºæœ¯é£æ ¼æ˜¯å¦ä¸€è‡´
- èƒŒæ™¯æ˜¯å¦ä¿æŒç™½è‰²

### **å›¾åƒè´¨é‡**
- åˆ†è¾¨ç‡å’Œæ¸…æ™°åº¦
- æ˜¯å¦æœ‰äººå·¥ç—•è¿¹
- æ•´ä½“è§†è§‰æ•ˆæœ

---

## âœ… ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³æµ‹è¯•**ï¼šè°ƒæ•´ IP-Adapter å‚æ•°
2. **å¹¶è¡Œæµ‹è¯•**ï¼šInstructPix2Pix åŸºç¡€ç‰ˆæœ¬
3. **æ·±å…¥ç ”ç©¶**ï¼šControlNet + IP-Adapter ç»„åˆ
4. **é•¿æœŸè§„åˆ’**ï¼šè¯„ä¼°æ–°æ¨¡å‹å’Œä¸‹è½½éƒ¨ç½²

é‡ç‚¹è§£å†³å½“å‰**ç¼–è¾‘èƒ½åŠ›ä¸è¶³**å’Œ**é¢œè‰²åç§»**é—®é¢˜ï¼Œé€æ­¥æå‡æŒ‡ä»¤ç¼–è¾‘çš„ç²¾å‡†åº¦ã€‚
