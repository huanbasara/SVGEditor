# Diffusion å›¾åƒç¼–è¾‘æ–¹æ¡ˆå‚è€ƒ

## ğŸ¯ ç›®æ ‡

- **è¾“å…¥**ï¼šå›¾åƒï¼ˆinit imageï¼‰ + æŒ‡ä»¤ï¼ˆinstruction promptï¼‰
- **è¦æ±‚**ï¼šç²¾å‡†ç¼–è¾‘ï¼Œä¿æŒåŸå›¾é£æ ¼ï¼ˆå°¤å…¶æ˜¯åŠ¨ç”»/çº¯è‰²å—é£æ ¼ï¼‰
- **æ–¹æ³•**ï¼šä½¿ç”¨ Stable Diffusion (SDXL / SD1.5) çš„ **img2img** ç®¡çº¿ï¼Œç»“åˆ
  - **ControlNet** â†’ é”å®šç»“æ„
  - **IP-Adapter** â†’ ä¿æŒé£æ ¼

---

## ğŸ“¦ ç¯å¢ƒä¾èµ–

```bash
pip install -U diffusers transformers accelerate safetensors controlnet-aux
```

éœ€è¦ GPU (CUDA)ã€‚

---

## ğŸ…°ï¸ æ–¹æ¡ˆ A: SDXL + ControlNet + IP-Adapter + Img2Img

```python
import torch
from PIL import Image
from diffusers import StableDiffusionXLControlNetImg2ImgPipeline, ControlNetModel
from diffusers.utils import load_image

device = "cuda"

# 1) ControlNet (ç»“æ„æ¡ä»¶: canny/lineart/pose/seg)
controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0",
    torch_dtype=torch.float16
).to(device)

# 2) SDXL + ControlNet çš„ img2img ç®¡çº¿
pipe = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    controlnet=controlnet, torch_dtype=torch.float16
).to(device)

# 3) åŠ è½½ IP-Adapter (é£æ ¼å‚è€ƒ)
pipe.load_ip_adapter(
    "h94/IP-Adapter",
    subfolder="models",
    weight_name="ip-adapter-plus_sdxl_vit-h.bin"
)

# 4) è¾“å…¥
init_img   = Image.open("input.png").convert("RGB")      # åŸå›¾
control_im = Image.open("control.png").convert("RGB")    # ç»“æ„æ¡ä»¶å›¾ (çº¿ç¨¿/è¾¹ç¼˜)
ref_style  = Image.open("ref_style.png").convert("RGB")  # é£æ ¼å‚è€ƒå›¾

prompt = "Replace the left triangle with a red circle; flat colors; clean edges; anime style."
neg    = "photorealistic, texture, shading, gradients, noise, blur"

# 5) æ¨ç†
out = pipe(
    prompt=prompt,
    negative_prompt=neg,
    image=init_img,
    control_image=control_im,
    ip_adapter_image=ref_style,
    strength=0.35,
    guidance_scale=6.0,
    controlnet_conditioning_scale=0.9,
    ip_adapter_scale=0.9,
    num_inference_steps=30
)

out.images[0].save("edit_sdxl_cn_ip_i2i.png")
```

---

## ğŸ…±ï¸ æ–¹æ¡ˆ B: SD1.5 + ControlNet (Anime/Lineart) + IP-Adapter + Img2Img

> å¯¹åŠ¨ç”»/çº¯è‰²å—é£æ ¼ï¼ŒSD1.5 çš„ç”Ÿæ€ï¼ˆControlNet-Anime/Lineartï¼‰æ›´æˆç†Ÿã€‚

```python
import torch
from PIL import Image
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel

device = "cuda"
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11p_sd15s2_lineart_anime", torch_dtype=torch.float16
).to(device)

pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet,
    torch_dtype=torch.float16
).to(device)

# SD1.5 ç‰ˆ IP-Adapter
pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")

init_img   = Image.open("input.png").convert("RGB")
control_im = Image.open("lineart.png").convert("RGB")
ref_style  = Image.open("ref_style.png").convert("RGB")

prompt = "Change the left triangle to a red circle; flat colors; crisp lines; vector-like, no textures."
neg    = "photorealistic, detailed textures, shading, gradients, noise, blur"

out = pipe(
    prompt=prompt, negative_prompt=neg,
    image=init_img, control_image=control_im, ip_adapter_image=ref_style,
    strength=0.3, guidance_scale=6.5,
    controlnet_conditioning_scale=0.95, ip_adapter_scale=0.9,
    num_inference_steps=30
)

out.images[0].save("edit_sd15_cn_ip_i2i.png")
```

---

## âš™ï¸ è°ƒå‚è¦ç‚¹

- **åŒé”šæ³•**ï¼š
  - ç»“æ„é”š â†’ `control_image` + `controlnet_conditioning_scale`
  - é£æ ¼é”š â†’ `ip_adapter_image` + `ip_adapter_scale`
- **strength (img2img æ”¹åŠ¨å¹…åº¦)**ï¼š
  - 0.2â€“0.4 æ›´è´´è¿‘åŸå›¾
  -
    > 0.6 å®¹æ˜“é£æ ¼è·‘å
- **è´Ÿæç¤º (negative prompt)**ï¼š
  - å¼ºæ’å†™å®åŒ–ï¼š`photorealistic, texture, shading, gradients, grain, blur`
- **inpaint æ›´ç¨³**ï¼šå±€éƒ¨æ”¹åŠ¨å¯æ¢ `InpaintPipeline`ï¼ŒåŠ  `mask_image`
- **æ¨¡å‹é€‰æ‹©**ï¼š
  - **SDXL**ï¼šæ›´é«˜åˆ†è¾¨ç‡ï¼Œä½†åŠ¨ç”»é£æ ¼ç›¸å¯¹å°‘
  - **SD1.5**ï¼šç¤¾åŒº anime/flat-color æ¨¡å‹å¤šï¼Œé£æ ¼æ›´å®¹æ˜“ç¨³ä½

---

## ğŸ” æ£€ç´¢ä¸èµ„æº

- **Hugging Face Models** æœç´¢å…³é”®è¯ï¼š

  - `controlnet lineart anime`
  - `controlnet canny sdxl`
  - `t2i-adapter lineart sdxl`
  - `ip-adapter sdxl`
  - `anime flat color`
  - `instruction image editing`

- **æ¨èä½œè€…/ç»„ç»‡**ï¼š

  - `lllyasviel` â†’ ControlNet ç³»åˆ—
  - `TencentARC` â†’ T2I-Adapter, MasaCtrl
  - `instantX-research` â†’ InstantStyle

- **GitHub é¡¹ç›®**ï¼š

  - [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)
  - [InstantStyle](https://github.com/instantX-research/InstantStyle)
  - [T2I-Adapter](https://github.com/TencentARC/T2I-Adapter)
  - [Prompt-to-Prompt](https://github.com/google/prompt-to-prompt)
  - [MasaCtrl](https://github.com/TencentARC/MasaCtrl)

---

## âœ… æ€»ç»“

- ç”¨ **img2img** ä¿è¯â€œå›¾åƒ+æŒ‡ä»¤â€è¾“å…¥æ¨¡å¼ã€‚
- **ControlNet** ä¿ç»“æ„ï¼Œ**IP-Adapter** ä¿é£æ ¼ã€‚
- SD1.5 æ›´é€‚åˆåŠ¨ç”»/çº¯è‰²å—ï¼ŒSDXL åˆ†è¾¨ç‡é«˜ä½†é£æ ¼æ¨¡å‹å°‘ã€‚
- è°ƒå¥½ `strength`ã€`guidance`ã€`scale` ä¸‰ç±»å‚æ•°ï¼Œå°±èƒ½åœ¨â€œé£æ ¼ä¸è·‘åâ€å’Œâ€œæŒ‰æŒ‡ä»¤ç¼–è¾‘â€ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

